{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e41ba73-06bb-42c6-8693-7ca41c2d0fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from albumentations) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from albumentations) (1.15.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Collecting pydantic>=2.9.2 (from albumentations)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting albucore==0.0.23 (from albumentations)\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.23->albumentations)\n",
      "  Downloading stringzilla-3.12.4-cp313-cp313-win_amd64.whl.metadata (81 kB)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n",
      "  Downloading simsimd-6.2.1-cp313-cp313-win_amd64.whl.metadata (67 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading pydantic_core-2.33.1-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.13.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n",
      "Downloading albucore-0.0.23-py3-none-any.whl (14 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/39.4 MB 6.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.6/39.4 MB 6.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 3.9/39.4 MB 6.4 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 5.5/39.4 MB 6.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 6.8/39.4 MB 6.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 8.1/39.4 MB 6.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 9.7/39.4 MB 6.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.0/39.4 MB 6.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.6/39.4 MB 6.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 14.4/39.4 MB 6.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 16.0/39.4 MB 6.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 17.3/39.4 MB 6.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.9/39.4 MB 6.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 20.4/39.4 MB 6.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 22.0/39.4 MB 7.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 23.3/39.4 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 25.2/39.4 MB 7.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 26.7/39.4 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 28.0/39.4 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.6/39.4 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 31.2/39.4 MB 7.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.8/39.4 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 34.1/39.4 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.7/39.4 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 37.2/39.4 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 38.3/39.4 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.0/2.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading simsimd-6.2.1-cp313-cp313-win_amd64.whl (87 kB)\n",
      "Downloading stringzilla-3.12.4-cp313-cp313-win_amd64.whl (80 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: stringzilla, simsimd, typing-inspection, pydantic-core, opencv-python-headless, annotated-types, pydantic, albucore, albumentations\n",
      "Successfully installed albucore-0.0.23 albumentations-2.0.5 annotated-types-0.7.0 opencv-python-headless-4.11.0.86 pydantic-2.11.3 pydantic-core-2.33.1 simsimd-6.2.1 stringzilla-3.12.4 typing-inspection-0.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da4e477-6025-428b-b30e-9a717c99ca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (2.6.0)\n",
      "Collecting torchvision (from timm)\n",
      "  Downloading torchvision-0.21.0-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (6.0.2)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub->timm)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->timm) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aditya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading torchvision-0.21.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, huggingface_hub, torchvision, timm\n",
      "Successfully installed huggingface_hub-0.30.2 safetensors-0.5.3 timm-1.0.15 torchvision-0.21.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07302b9e-70bf-41fc-8d29-136a59016272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Ensemble Results:\n",
      "Accuracy:  0.9680\n",
      "Precision: 0.9680\n",
      "Recall:    0.9680\n",
      "F1-score:  0.9680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ✅ Step 1: Load CSV\n",
    "df = pd.read_csv(\"D:/fy_project1/data3.csv\")  # Replace with your filename if different\n",
    "\n",
    "# ✅ Step 2: Define the target column\n",
    "target_col = 'category'\n",
    "\n",
    "# ✅ Step 3: Clean the label values\n",
    "df[target_col] = df[target_col].astype(str).str.strip()\n",
    "\n",
    "# ✅ Step 4: Drop image_name column (it's metadata, not a feature)\n",
    "df = df.drop(columns=['image_name'])\n",
    "\n",
    "# ✅ Step 5: Remove rare classes (those with only 1 sample)\n",
    "class_counts = df[target_col].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[target_col].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 6: Extract features and target\n",
    "X = filtered_df.drop(columns=[target_col])\n",
    "y = filtered_df[target_col]\n",
    "\n",
    "# ✅ Step 7: Define attention module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 8: Stacking Ensemble Class\n",
    "class StackingWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        self.base_models = [\n",
    "            ('svm', SVC(kernel='rbf', C=10, probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=7, random_state=42)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), early_stopping=True))\n",
    "        ]\n",
    "        \n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=10, max_iter=1000),\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        self.attention.train()\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        \n",
    "        self.meta_learner.fit(X_att_np, y_encoded)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        return self.meta_learner.predict_proba(X_att_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "weather_ensemble = StackingWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Ensemble Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ef90ad-b9c4-4d76-860d-aed9984264a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Results:\n",
      "Accuracy:  0.9776\n",
      "Precision: 0.9776\n",
      "Recall:    0.9776\n",
      "F1-score:  0.9776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data3.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])  # Drop metadata column\n",
    "\n",
    "# ✅ Step 2: Filter out rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Step 4: CNN + Transformer Model\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)        # (batch, 1, features)\n",
    "        x = self.conv(x)          # (batch, 128, features//2)\n",
    "        x = x.permute(2, 0, 1)    # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ✅ Step 5: Scikit-learn compatible wrapper\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=self.num_classes_)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# ✅ Step 6: Ensemble definition\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                tree_method='hist', eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=5,\n",
    "            stack_method='auto',\n",
    "            n_jobs=1  # ⚠️ Use 1 to avoid joblib multiprocessing issues with PyTorch\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.meta_learner.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.meta_learner.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.meta_learner.predict(X)\n",
    "\n",
    "# ✅ Step 7: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 8: Train and Evaluate\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 9: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef312838-3f9a-4257-a01f-810ffeb008b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Ensemble Results:\n",
      "Accuracy:  0.9417\n",
      "Precision: 0.9354\n",
      "Recall:    0.9417\n",
      "F1-score:  0.9378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ✅ Step 1: Load CSV\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")  # Replace with your filename if different\n",
    "\n",
    "# ✅ Step 2: Define the target column\n",
    "target_col = 'category'\n",
    "\n",
    "# ✅ Step 3: Clean the label values\n",
    "df[target_col] = df[target_col].astype(str).str.strip()\n",
    "\n",
    "# ✅ Step 4: Drop image_name column (it's metadata, not a feature)\n",
    "df = df.drop(columns=['image_name'])\n",
    "\n",
    "# ✅ Step 5: Remove rare classes (those with only 1 sample)\n",
    "class_counts = df[target_col].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[target_col].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 6: Extract features and target\n",
    "X = filtered_df.drop(columns=[target_col])\n",
    "y = filtered_df[target_col]\n",
    "\n",
    "# ✅ Step 7: Define attention module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 8: Stacking Ensemble Class\n",
    "class StackingWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        self.base_models = [\n",
    "            ('svm', SVC(kernel='rbf', C=10, probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=7, random_state=42)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), early_stopping=True))\n",
    "        ]\n",
    "        \n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=10, max_iter=1000),\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        self.attention.train()\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        \n",
    "        self.meta_learner.fit(X_att_np, y_encoded)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        return self.meta_learner.predict_proba(X_att_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "weather_ensemble = StackingWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Ensemble Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6692846-0eec-47c9-bb7a-df3f15148d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Results:\n",
      "Accuracy:  0.9507\n",
      "Precision: 0.9473\n",
      "Recall:    0.9507\n",
      "F1-score:  0.9485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])\n",
    "\n",
    "# Step 2: Filter rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# Step 4: Updated HybridCNNTransformer\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128, nhead=4, dim_feedforward=256, dropout=0.1,\n",
    "            activation=\"gelu\", batch_first=False, norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=3)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        x = self.conv(x)    # (batch, 128, features//4)\n",
    "        x = x.permute(2, 0, 1)  # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Step 5: Sklearn wrapper\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=50, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=self.num_classes_)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# Step 6: Ensemble definition\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                tree_method='hist', eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=5,\n",
    "            stack_method='auto',\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.meta_learner.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.meta_learner.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.meta_learner.predict(X)\n",
    "\n",
    "# Step 7: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 8: Train and evaluate\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# Step 9: Print metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97fba57-6266-4886-a227-148beb4cb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Attention + Manual MetaLearner Results:\n",
      "Accuracy:  0.9395\n",
      "Precision: 0.9355\n",
      "Recall:    0.9395\n",
      "F1-score:  0.9369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 0: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "import torch.optim as optim\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])\n",
    "\n",
    "# ✅ Step 2: Filter out rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Step 4: Enhanced Weather Attention Module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 5: Hybrid CNN + Transformer Model\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ✅ Step 6: Wrapper for Hybrid Model\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=len(self.classes_))\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# ✅ Step 7: Neural Network Meta-Learner\n",
    "class MetaLearnerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "class NNMetaLearner(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.001, epochs=100, batch_size=32):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = MetaLearnerNN(self.input_dim, self.hidden_dim, self.output_dim)\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, ['model'])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "# ✅ Step 8: Ensemble Class with Manual Meta Learner\n",
    "class EnhancedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features, num_classes):\n",
    "        self.n_features = n_features\n",
    "        self.num_classes = num_classes\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.base_models = [\n",
    "            ('hybrid', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                                  tree_method='hist', eval_metric='mlogloss')),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(256, 128), activation='relu',\n",
    "                                  early_stopping=True, batch_size=256))\n",
    "        ]\n",
    "        self.meta_learner = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.to_numpy() if isinstance(X, pd.DataFrame) else X)\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_transformed = self.attention(X_tensor).numpy()\n",
    "\n",
    "        self.fitted_base_models_ = []\n",
    "        base_outputs = []\n",
    "        for name, model in self.base_models:\n",
    "            model.fit(X_transformed, y_enc)\n",
    "            self.fitted_base_models_.append((name, model))\n",
    "            base_outputs.append(model.predict_proba(X_transformed))\n",
    "\n",
    "        meta_input = np.hstack(base_outputs)\n",
    "        self.meta_learner = NNMetaLearner(\n",
    "            input_dim=meta_input.shape[1], hidden_dim=128, output_dim=self.num_classes\n",
    "        )\n",
    "        self.meta_learner.fit(meta_input, y_enc)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.to_numpy() if isinstance(X, pd.DataFrame) else X)\n",
    "        with torch.no_grad():\n",
    "            X_transformed = self.attention(X_tensor).numpy()\n",
    "        base_outputs = [model.predict_proba(X_transformed) for _, model in self.fitted_base_models_]\n",
    "        meta_input = np.hstack(base_outputs)\n",
    "        return self.meta_learner.predict_proba(meta_input)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "ensemble = EnhancedWeatherEnsemble(n_features=X.shape[1], num_classes=len(np.unique(y)))\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Print metrics\n",
    "print(f'''\n",
    "Enhanced Attention + Manual MetaLearner Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8009896f-67b3-44da-aa7a-d64977fbe46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Enhanced Weather Ensemble Results:\n",
      "    Accuracy:  0.9469\n",
      "    Precision: 0.9434\n",
      "    Recall:    0.9469\n",
      "    F1-score:  0.9420\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 0: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "import torch.optim as optim\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])\n",
    "\n",
    "# ✅ Step 2: Filter out rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Enhanced Weather Attention Module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features//2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features//2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(self.input_dim, self.num_classes_)\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, ['model'])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "class MetaLearnerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# ✅ Train and Evaluate\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    n_features = X_train.shape[1]\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_enc = label_encoder.fit_transform(y_train)\n",
    "    y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "    # Step 1: Apply attention\n",
    "    attention = EnhancedWeatherAttention(n_features)\n",
    "    optimizer = torch.optim.Adam(attention.parameters(), lr=0.001)\n",
    "    X_tensor = torch.FloatTensor(X_train.values)\n",
    "    for _ in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        X_att = attention(X_tensor)\n",
    "        loss = F.mse_loss(X_att, X_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_train_att = attention(torch.FloatTensor(X_train.values)).numpy()\n",
    "        X_test_att = attention(torch.FloatTensor(X_test.values)).numpy()\n",
    "\n",
    "    # Step 2: Train base models\n",
    "    base_preds_train = []\n",
    "    base_preds_test = []\n",
    "\n",
    "    base_models = [\n",
    "        HybridWrapper(n_features).fit(X_train_att, y_train),\n",
    "        XGBClassifier(n_estimators=500, max_depth=9, learning_rate=0.05, tree_method='hist', eval_metric='mlogloss').fit(X_train_att, y_train_enc),\n",
    "        MLPClassifier(hidden_layer_sizes=(256, 128), activation='relu', early_stopping=True, batch_size=256).fit(X_train_att, y_train_enc)\n",
    "    ]\n",
    "\n",
    "    for model in base_models:\n",
    "        base_preds_train.append(model.predict_proba(X_train_att))\n",
    "        base_preds_test.append(model.predict_proba(X_test_att))\n",
    "\n",
    "    # Step 3: Prepare meta-input\n",
    "    meta_X_train = np.hstack(base_preds_train)\n",
    "    meta_X_test = np.hstack(base_preds_test)\n",
    "\n",
    "    # Step 4: Train NNMetaLearner\n",
    "    meta_learner = MetaLearnerNN(input_dim=meta_X_train.shape[1], hidden_dim=128, output_dim=num_classes)\n",
    "    optimizer = optim.Adam(meta_learner.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    dataset = TensorDataset(torch.FloatTensor(meta_X_train), torch.LongTensor(y_train_enc))\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    for epoch in range(100):\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = meta_learner(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Step 5: Evaluate\n",
    "    with torch.no_grad():\n",
    "        outputs = meta_learner(torch.FloatTensor(meta_X_test))\n",
    "        y_pred = np.argmax(F.softmax(outputs, dim=1).numpy(), axis=1)\n",
    "    print(f'''\n",
    "    Enhanced Weather Ensemble Results:\n",
    "    Accuracy:  {accuracy_score(y_test_enc, y_pred):.4f}\n",
    "    Precision: {precision_score(y_test_enc, y_pred, average='weighted'):.4f}\n",
    "    Recall:    {recall_score(y_test_enc, y_pred, average='weighted'):.4f}\n",
    "    F1-score:  {f1_score(y_test_enc, y_pred, average='weighted'):.4f}\n",
    "    ''')\n",
    "\n",
    "# ✅ Data Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "model = train_and_evaluate(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6cd650b-8d8a-49c6-94c9-44ce3a8bcf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\decorators.py:42: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\decorators.py:42: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\decorators.py:42: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\decorators.py:42: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\decorators.py:42: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albucore\\decorators.py:42: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Results with Augmentation:\n",
      "Accuracy:  0.9537\n",
      "Precision: 0.9504\n",
      "Recall:    0.9537\n",
      "F1-score:  0.9515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "import albumentations as A\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])  # Drop metadata column\n",
    "\n",
    "# ✅ Step 2: Filter rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Step 4: Data augmentation using Albumentations (real-world conditions)\n",
    "augmenter = A.Compose([\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "    A.HueSaturationValue(p=0.2),\n",
    "])\n",
    "\n",
    "def augment_data(X):\n",
    "    X_aug = []\n",
    "    for x in X:\n",
    "        img = np.array(x, dtype=np.float32).reshape(1, -1)\n",
    "        augmented = augmenter(image=img)[\"image\"]\n",
    "        X_aug.append(augmented.flatten())\n",
    "    return np.array(X_aug)\n",
    "\n",
    "# ✅ Step 5: CNN + Transformer Model\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)        # (batch, 1, features)\n",
    "        x = self.conv(x)          # (batch, 128, features//2)\n",
    "        x = x.permute(2, 0, 1)    # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ✅ Step 6: Wrapper for Hybrid model\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        # Augment training data\n",
    "        X_augmented = augment_data(X)\n",
    "\n",
    "        # Combine original and augmented data\n",
    "        X_combined = np.vstack([X, X_augmented])\n",
    "        y_combined = np.concatenate([y, y])\n",
    "\n",
    "        y_enc = self.label_encoder.fit_transform(y_combined)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=self.num_classes_)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X_combined)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# ✅ Step 7: Ensemble definition\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                tree_method='hist', eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=5,\n",
    "            stack_method='auto',\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.meta_learner.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.meta_learner.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.meta_learner.predict(X)\n",
    "\n",
    "# ✅ Step 8: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 9: Train and Evaluate\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 10: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Results with Augmentation:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ed7bd14-f7a3-41c6-a381-57fbfb94d036",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 173888 into shape (224,224,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m y = filtered_df[\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m].values\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Assume images are 224x224x3 (Reshape your actual data accordingly)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m X_images = \u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# ✅ Step 3: Data Augmentation\u001b[39;00m\n\u001b[32m     35\u001b[39m augmenter = A.Compose([\n\u001b[32m     36\u001b[39m     A.HorizontalFlip(p=\u001b[32m0.5\u001b[39m),\n\u001b[32m     37\u001b[39m     A.VerticalFlip(p=\u001b[32m0.5\u001b[39m),\n\u001b[32m     38\u001b[39m     A.RandomBrightnessContrast(p=\u001b[32m0.3\u001b[39m),\n\u001b[32m     39\u001b[39m     A.GaussianBlur(p=\u001b[32m0.2\u001b[39m),\n\u001b[32m     40\u001b[39m ])\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 173888 into shape (224,224,3)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import albumentations as A\n",
    "import timm\n",
    "\n",
    "# ✅ Step 1: Load Dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])\n",
    "\n",
    "# ✅ Step 2: Filter Rare Classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "X = filtered_df.drop(columns=[\"category\"]).values\n",
    "y = filtered_df[\"category\"].values\n",
    "\n",
    "# Assume images are 224x224x3 (Reshape your actual data accordingly)\n",
    "X_images = X.reshape(-1, 224, 224, 3)\n",
    "\n",
    "# ✅ Step 3: Data Augmentation\n",
    "augmenter = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "])\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, images, labels, augment=False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.le = LabelEncoder()\n",
    "        self.labels_encoded = self.le.fit_transform(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels_encoded[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            img = augmenter(image=img)['image']\n",
    "\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = np.transpose(img, (2, 0, 1))  # To channel-first (C,H,W)\n",
    "        return torch.tensor(img), torch.tensor(label)\n",
    "\n",
    "# ✅ Step 4: Hybrid EfficientNet + Transformer Model\n",
    "class EfficientTransformer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('efficientnet_b3', pretrained=True, num_classes=0)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=1536, nhead=8, dim_feedforward=1024),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.classifier = nn.Linear(1536, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ✅ Step 5: Wrapper\n",
    "class EfficientWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, epochs=10, lr=0.0001, batch_size=16):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        num_classes = len(self.classes_)\n",
    "\n",
    "        train_dataset = WeatherDataset(X, y, augment=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model = EfficientTransformer(num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for imgs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        dataset = WeatherDataset(X, np.zeros(len(X)))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "        self.model.eval()\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in loader:\n",
    "                outputs = self.model(imgs)\n",
    "                probs.append(F.softmax(outputs, dim=1).numpy())\n",
    "        return np.vstack(probs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# ✅ Step 6: Ensemble\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.base_models = [\n",
    "            ('efficient_transformer', EfficientWrapper()),\n",
    "            ('xgb', XGBClassifier(n_estimators=500, max_depth=9, learning_rate=0.05, tree_method='hist')),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(256,128), activation='relu', early_stopping=True))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=3,\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_flat = X.reshape(len(X), -1)\n",
    "        self.meta_learner.fit(X_flat, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_flat = X.reshape(len(X), -1)\n",
    "        return self.meta_learner.predict(X_flat)\n",
    "\n",
    "# ✅ Step 7: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_images, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 8: Train & Evaluate\n",
    "ensemble = ImprovedWeatherEnsemble()\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 9: Metrics\n",
    "print(f'''\n",
    "EfficientNet Transformer Ensemble Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42521c-f5ba-4013-b15a-b038b35aa68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
