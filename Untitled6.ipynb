{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4213ee7-f6b2-4278-bb49-108aee1a3423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Ensemble Results:\n",
      "Accuracy:  0.9740\n",
      "Precision: 0.9740\n",
      "Recall:    0.9740\n",
      "F1-score:  0.9740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ✅ Step 1: Load CSV\n",
    "df = pd.read_csv(\"D:/fy_project1/data3.csv\")  # Replace with your filename if different\n",
    "\n",
    "# ✅ Step 2: Define the target column\n",
    "target_col = 'category'\n",
    "\n",
    "# ✅ Step 3: Clean the label values\n",
    "df[target_col] = df[target_col].astype(str).str.strip()\n",
    "\n",
    "# ✅ Step 4: Drop image_name column (it's metadata, not a feature)\n",
    "df = df.drop(columns=['image_name'])\n",
    "\n",
    "# ✅ Step 5: Remove rare classes (those with only 1 sample)\n",
    "class_counts = df[target_col].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[target_col].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 6: Extract features and target\n",
    "X = filtered_df.drop(columns=[target_col])\n",
    "y = filtered_df[target_col]\n",
    "\n",
    "# ✅ Step 7: Define attention module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 8: Stacking Ensemble Class\n",
    "class StackingWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        self.base_models = [\n",
    "            ('svm', SVC(kernel='rbf', C=10, probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=7, random_state=42)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), early_stopping=True))\n",
    "        ]\n",
    "        \n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=10, max_iter=1000),\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        self.attention.train()\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        \n",
    "        self.meta_learner.fit(X_att_np, y_encoded)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        return self.meta_learner.predict_proba(X_att_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "weather_ensemble = StackingWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Ensemble Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc029790-df17-4aa4-9d76-5ec2a0752c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Results with NN Meta-Learner:\n",
      "Accuracy:  0.9722\n",
      "Precision: 0.9721\n",
      "Recall:    0.9722\n",
      "F1-score:  0.9722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class MetaLearnerNN(nn.Module):\n",
    "    def _init_(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MetaLearnerNN, self)._init_()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "class NNMetaLearner(BaseEstimator, ClassifierMixin):\n",
    "    def _init_(self, input_dim, hidden_dim, output_dim, lr=0.001, epochs=100, batch_size=32):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = MetaLearnerNN(input_dim, hidden_dim, output_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "            return self.model(X_tensor).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def _init_(self, input_dim, num_classes):\n",
    "        super()._init_()\n",
    "        # 1D CNN for local pattern extraction\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Transformer for global dependencies\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=128,\n",
    "                nhead=4,\n",
    "                dim_feedforward=256\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        # Weather-specific attention fusion\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, features)\n",
    "        x = x.unsqueeze(1)  # Add channel dim: (batch, 1, features)\n",
    "        x = self.conv(x)    # (batch, 128, features//2)\n",
    "        x = x.permute(2, 0, 1)  # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)   # Global average pooling\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x  # Residual attention\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def _init_(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.classes_ = None  # Add this line\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_  # Set classes_ attribute\n",
    "        \n",
    "        # Rest of fit method remains the same\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(\n",
    "            input_dim=self.input_dim,\n",
    "            num_classes=self.num_classes_\n",
    "        )\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        \n",
    "         # Training setup\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    # Keep existing predict_proba/predict methods\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, ['model'])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'input_dim': self.input_dim, 'epochs': self.epochs, 'lr': self.lr}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "class ImprovedWeatherEnsemble(StackingWeatherEnsemble):\n",
    "    def _init_(self, n_features=14):\n",
    "        super()._init_(n_features)\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=9,\n",
    "                learning_rate=0.05,\n",
    "                tree_method='hist',\n",
    "                eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        # Calculate input dimension for meta-learner\n",
    "        num_classes = len(np.unique(y))  # Assuming 'y' is available\n",
    "        input_dim = len(self.base_models) * num_classes\n",
    "        \n",
    "        # Replace LogisticRegression with NNMetaLearner\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=NNMetaLearner(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=128,\n",
    "                output_dim=num_classes,\n",
    "                lr=0.001,\n",
    "                epochs=100,\n",
    "                batch_size=32\n",
    "            ),\n",
    "            cv=7,\n",
    "            stack_method='predict_proba',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "# Initialize and train with enhanced preprocessing\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "print(f'''\n",
    "Enhanced Stacking Results with NN Meta-Learner:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979611a1-3cfb-433e-8bab-929151723b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
