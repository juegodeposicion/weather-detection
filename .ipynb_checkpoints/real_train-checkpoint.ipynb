{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07302b9e-70bf-41fc-8d29-136a59016272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Ensemble Results:\n",
      "Accuracy:  0.9680\n",
      "Precision: 0.9680\n",
      "Recall:    0.9680\n",
      "F1-score:  0.9680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ✅ Step 1: Load CSV\n",
    "df = pd.read_csv(\"D:/fy_project1/data3.csv\")  # Replace with your filename if different\n",
    "\n",
    "# ✅ Step 2: Define the target column\n",
    "target_col = 'category'\n",
    "\n",
    "# ✅ Step 3: Clean the label values\n",
    "df[target_col] = df[target_col].astype(str).str.strip()\n",
    "\n",
    "# ✅ Step 4: Drop image_name column (it's metadata, not a feature)\n",
    "df = df.drop(columns=['image_name'])\n",
    "\n",
    "# ✅ Step 5: Remove rare classes (those with only 1 sample)\n",
    "class_counts = df[target_col].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[target_col].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 6: Extract features and target\n",
    "X = filtered_df.drop(columns=[target_col])\n",
    "y = filtered_df[target_col]\n",
    "\n",
    "# ✅ Step 7: Define attention module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 8: Stacking Ensemble Class\n",
    "class StackingWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        self.base_models = [\n",
    "            ('svm', SVC(kernel='rbf', C=10, probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=7, random_state=42)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), early_stopping=True))\n",
    "        ]\n",
    "        \n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=10, max_iter=1000),\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        self.attention.train()\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        \n",
    "        self.meta_learner.fit(X_att_np, y_encoded)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        return self.meta_learner.predict_proba(X_att_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "weather_ensemble = StackingWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Ensemble Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ef90ad-b9c4-4d76-860d-aed9984264a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Results:\n",
      "Accuracy:  0.9776\n",
      "Precision: 0.9776\n",
      "Recall:    0.9776\n",
      "F1-score:  0.9776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data3.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])  # Drop metadata column\n",
    "\n",
    "# ✅ Step 2: Filter out rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Step 4: CNN + Transformer Model\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)        # (batch, 1, features)\n",
    "        x = self.conv(x)          # (batch, 128, features//2)\n",
    "        x = x.permute(2, 0, 1)    # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ✅ Step 5: Scikit-learn compatible wrapper\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=self.num_classes_)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# ✅ Step 6: Ensemble definition\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                tree_method='hist', eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=5,\n",
    "            stack_method='auto',\n",
    "            n_jobs=1  # ⚠️ Use 1 to avoid joblib multiprocessing issues with PyTorch\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.meta_learner.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.meta_learner.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.meta_learner.predict(X)\n",
    "\n",
    "# ✅ Step 7: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 8: Train and Evaluate\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 9: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef312838-3f9a-4257-a01f-810ffeb008b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Ensemble Results:\n",
      "Accuracy:  0.9417\n",
      "Precision: 0.9354\n",
      "Recall:    0.9417\n",
      "F1-score:  0.9378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ✅ Step 1: Load CSV\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")  # Replace with your filename if different\n",
    "\n",
    "# ✅ Step 2: Define the target column\n",
    "target_col = 'category'\n",
    "\n",
    "# ✅ Step 3: Clean the label values\n",
    "df[target_col] = df[target_col].astype(str).str.strip()\n",
    "\n",
    "# ✅ Step 4: Drop image_name column (it's metadata, not a feature)\n",
    "df = df.drop(columns=['image_name'])\n",
    "\n",
    "# ✅ Step 5: Remove rare classes (those with only 1 sample)\n",
    "class_counts = df[target_col].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[target_col].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 6: Extract features and target\n",
    "X = filtered_df.drop(columns=[target_col])\n",
    "y = filtered_df[target_col]\n",
    "\n",
    "# ✅ Step 7: Define attention module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 8: Stacking Ensemble Class\n",
    "class StackingWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        self.base_models = [\n",
    "            ('svm', SVC(kernel='rbf', C=10, probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=7, random_state=42)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), early_stopping=True))\n",
    "        ]\n",
    "        \n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=10, max_iter=1000),\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        self.attention.train()\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        \n",
    "        self.meta_learner.fit(X_att_np, y_encoded)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        return self.meta_learner.predict_proba(X_att_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "weather_ensemble = StackingWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Ensemble Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6692846-0eec-47c9-bb7a-df3f15148d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Results:\n",
      "Accuracy:  0.9507\n",
      "Precision: 0.9473\n",
      "Recall:    0.9507\n",
      "F1-score:  0.9485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])\n",
    "\n",
    "# Step 2: Filter rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# Step 4: Updated HybridCNNTransformer\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128, nhead=4, dim_feedforward=256, dropout=0.1,\n",
    "            activation=\"gelu\", batch_first=False, norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=3)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        x = self.conv(x)    # (batch, 128, features//4)\n",
    "        x = x.permute(2, 0, 1)  # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Step 5: Sklearn wrapper\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=50, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=self.num_classes_)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# Step 6: Ensemble definition\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                tree_method='hist', eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=5,\n",
    "            stack_method='auto',\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.meta_learner.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.meta_learner.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.meta_learner.predict(X)\n",
    "\n",
    "# Step 7: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 8: Train and evaluate\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# Step 9: Print metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97fba57-6266-4886-a227-148beb4cb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Attention + Manual MetaLearner Results:\n",
      "Accuracy:  0.9395\n",
      "Precision: 0.9355\n",
      "Recall:    0.9395\n",
      "F1-score:  0.9369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 0: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "import torch.optim as optim\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])\n",
    "\n",
    "# ✅ Step 2: Filter out rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Step 4: Enhanced Weather Attention Module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 5: Hybrid CNN + Transformer Model\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ✅ Step 6: Wrapper for Hybrid Model\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=len(self.classes_))\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# ✅ Step 7: Neural Network Meta-Learner\n",
    "class MetaLearnerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "class NNMetaLearner(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.001, epochs=100, batch_size=32):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = MetaLearnerNN(self.input_dim, self.hidden_dim, self.output_dim)\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, ['model'])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "# ✅ Step 8: Ensemble Class with Manual Meta Learner\n",
    "class EnhancedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features, num_classes):\n",
    "        self.n_features = n_features\n",
    "        self.num_classes = num_classes\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.base_models = [\n",
    "            ('hybrid', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                                  tree_method='hist', eval_metric='mlogloss')),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(256, 128), activation='relu',\n",
    "                                  early_stopping=True, batch_size=256))\n",
    "        ]\n",
    "        self.meta_learner = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.to_numpy() if isinstance(X, pd.DataFrame) else X)\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_transformed = self.attention(X_tensor).numpy()\n",
    "\n",
    "        self.fitted_base_models_ = []\n",
    "        base_outputs = []\n",
    "        for name, model in self.base_models:\n",
    "            model.fit(X_transformed, y_enc)\n",
    "            self.fitted_base_models_.append((name, model))\n",
    "            base_outputs.append(model.predict_proba(X_transformed))\n",
    "\n",
    "        meta_input = np.hstack(base_outputs)\n",
    "        self.meta_learner = NNMetaLearner(\n",
    "            input_dim=meta_input.shape[1], hidden_dim=128, output_dim=self.num_classes\n",
    "        )\n",
    "        self.meta_learner.fit(meta_input, y_enc)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.to_numpy() if isinstance(X, pd.DataFrame) else X)\n",
    "        with torch.no_grad():\n",
    "            X_transformed = self.attention(X_tensor).numpy()\n",
    "        base_outputs = [model.predict_proba(X_transformed) for _, model in self.fitted_base_models_]\n",
    "        meta_input = np.hstack(base_outputs)\n",
    "        return self.meta_learner.predict_proba(meta_input)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "ensemble = EnhancedWeatherEnsemble(n_features=X.shape[1], num_classes=len(np.unique(y)))\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Print metrics\n",
    "print(f'''\n",
    "Enhanced Attention + Manual MetaLearner Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8009896f-67b3-44da-aa7a-d64977fbe46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Enhanced Weather Ensemble Results:\n",
      "    Accuracy:  0.9469\n",
      "    Precision: 0.9434\n",
      "    Recall:    0.9469\n",
      "    F1-score:  0.9420\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 0: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "import torch.optim as optim\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])\n",
    "\n",
    "# ✅ Step 2: Filter out rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Enhanced Weather Attention Module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features//2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features//2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(self.input_dim, self.num_classes_)\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, ['model'])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "class MetaLearnerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# ✅ Train and Evaluate\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    n_features = X_train.shape[1]\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_enc = label_encoder.fit_transform(y_train)\n",
    "    y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "    # Step 1: Apply attention\n",
    "    attention = EnhancedWeatherAttention(n_features)\n",
    "    optimizer = torch.optim.Adam(attention.parameters(), lr=0.001)\n",
    "    X_tensor = torch.FloatTensor(X_train.values)\n",
    "    for _ in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        X_att = attention(X_tensor)\n",
    "        loss = F.mse_loss(X_att, X_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_train_att = attention(torch.FloatTensor(X_train.values)).numpy()\n",
    "        X_test_att = attention(torch.FloatTensor(X_test.values)).numpy()\n",
    "\n",
    "    # Step 2: Train base models\n",
    "    base_preds_train = []\n",
    "    base_preds_test = []\n",
    "\n",
    "    base_models = [\n",
    "        HybridWrapper(n_features).fit(X_train_att, y_train),\n",
    "        XGBClassifier(n_estimators=500, max_depth=9, learning_rate=0.05, tree_method='hist', eval_metric='mlogloss').fit(X_train_att, y_train_enc),\n",
    "        MLPClassifier(hidden_layer_sizes=(256, 128), activation='relu', early_stopping=True, batch_size=256).fit(X_train_att, y_train_enc)\n",
    "    ]\n",
    "\n",
    "    for model in base_models:\n",
    "        base_preds_train.append(model.predict_proba(X_train_att))\n",
    "        base_preds_test.append(model.predict_proba(X_test_att))\n",
    "\n",
    "    # Step 3: Prepare meta-input\n",
    "    meta_X_train = np.hstack(base_preds_train)\n",
    "    meta_X_test = np.hstack(base_preds_test)\n",
    "\n",
    "    # Step 4: Train NNMetaLearner\n",
    "    meta_learner = MetaLearnerNN(input_dim=meta_X_train.shape[1], hidden_dim=128, output_dim=num_classes)\n",
    "    optimizer = optim.Adam(meta_learner.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    dataset = TensorDataset(torch.FloatTensor(meta_X_train), torch.LongTensor(y_train_enc))\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    for epoch in range(100):\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = meta_learner(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Step 5: Evaluate\n",
    "    with torch.no_grad():\n",
    "        outputs = meta_learner(torch.FloatTensor(meta_X_test))\n",
    "        y_pred = np.argmax(F.softmax(outputs, dim=1).numpy(), axis=1)\n",
    "    print(f'''\n",
    "    Enhanced Weather Ensemble Results:\n",
    "    Accuracy:  {accuracy_score(y_test_enc, y_pred):.4f}\n",
    "    Precision: {precision_score(y_test_enc, y_pred, average='weighted'):.4f}\n",
    "    Recall:    {recall_score(y_test_enc, y_pred, average='weighted'):.4f}\n",
    "    F1-score:  {f1_score(y_test_enc, y_pred, average='weighted'):.4f}\n",
    "    ''')\n",
    "\n",
    "# ✅ Data Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "model = train_and_evaluate(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd650b-8d8a-49c6-94c9-44ce3a8bcf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "import albumentations as A\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data1.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])  # Drop metadata column\n",
    "\n",
    "# ✅ Step 2: Filter rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Step 4: Data augmentation using Albumentations (real-world conditions)\n",
    "augmenter = A.Compose([\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "    A.HueSaturationValue(p=0.2),\n",
    "])\n",
    "\n",
    "def augment_data(X):\n",
    "    X_aug = []\n",
    "    for x in X:\n",
    "        img = np.array(x, dtype=np.float32).reshape(1, -1)\n",
    "        augmented = augmenter(image=img)[\"image\"]\n",
    "        X_aug.append(augmented.flatten())\n",
    "    return np.array(X_aug)\n",
    "\n",
    "# ✅ Step 5: CNN + Transformer Model\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)        # (batch, 1, features)\n",
    "        x = self.conv(x)          # (batch, 128, features//2)\n",
    "        x = x.permute(2, 0, 1)    # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ✅ Step 6: Wrapper for Hybrid model\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        # Augment training data\n",
    "        X_augmented = augment_data(X)\n",
    "\n",
    "        # Combine original and augmented data\n",
    "        X_combined = np.vstack([X, X_augmented])\n",
    "        y_combined = np.concatenate([y, y])\n",
    "\n",
    "        y_enc = self.label_encoder.fit_transform(y_combined)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=self.num_classes_)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X_combined)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# ✅ Step 7: Ensemble definition\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                tree_method='hist', eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=5,\n",
    "            stack_method='auto',\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.meta_learner.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.meta_learner.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.meta_learner.predict(X)\n",
    "\n",
    "# ✅ Step 8: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 9: Train and Evaluate\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 10: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Results with Augmentation:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
