{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07302b9e-70bf-41fc-8d29-136a59016272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Ensemble Results:\n",
      "Accuracy:  0.9680\n",
      "Precision: 0.9680\n",
      "Recall:    0.9680\n",
      "F1-score:  0.9680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ✅ Step 1: Load CSV\n",
    "df = pd.read_csv(\"D:/fy_project1/data3.csv\")  # Replace with your filename if different\n",
    "\n",
    "# ✅ Step 2: Define the target column\n",
    "target_col = 'category'\n",
    "\n",
    "# ✅ Step 3: Clean the label values\n",
    "df[target_col] = df[target_col].astype(str).str.strip()\n",
    "\n",
    "# ✅ Step 4: Drop image_name column (it's metadata, not a feature)\n",
    "df = df.drop(columns=['image_name'])\n",
    "\n",
    "# ✅ Step 5: Remove rare classes (those with only 1 sample)\n",
    "class_counts = df[target_col].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[target_col].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 6: Extract features and target\n",
    "X = filtered_df.drop(columns=[target_col])\n",
    "y = filtered_df[target_col]\n",
    "\n",
    "# ✅ Step 7: Define attention module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 8: Stacking Ensemble Class\n",
    "class StackingWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        self.base_models = [\n",
    "            ('svm', SVC(kernel='rbf', C=10, probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=7, random_state=42)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), early_stopping=True))\n",
    "        ]\n",
    "        \n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=10, max_iter=1000),\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        self.attention.train()\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        \n",
    "        self.meta_learner.fit(X_att_np, y_encoded)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        return self.meta_learner.predict_proba(X_att_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "weather_ensemble = StackingWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Ensemble Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ef90ad-b9c4-4d76-860d-aed9984264a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Results:\n",
      "Accuracy:  0.9776\n",
      "Precision: 0.9776\n",
      "Recall:    0.9776\n",
      "F1-score:  0.9776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ✅ Step 1: Load and clean the dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data3.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])  # Drop metadata column\n",
    "\n",
    "# ✅ Step 2: Filter out rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# ✅ Step 4: CNN + Transformer Model\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=4, dim_feedforward=256),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)        # (batch, 1, features)\n",
    "        x = self.conv(x)          # (batch, 128, features//2)\n",
    "        x = x.permute(2, 0, 1)    # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ✅ Step 5: Scikit-learn compatible wrapper\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=20, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=self.num_classes_)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# ✅ Step 6: Ensemble definition\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                tree_method='hist', eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=5,\n",
    "            stack_method='auto',\n",
    "            n_jobs=1  # ⚠️ Use 1 to avoid joblib multiprocessing issues with PyTorch\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.meta_learner.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.meta_learner.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.meta_learner.predict(X)\n",
    "\n",
    "# ✅ Step 7: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 8: Train and Evaluate\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 9: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef312838-3f9a-4257-a01f-810ffeb008b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Ensemble Results:\n",
      "Accuracy:  0.9417\n",
      "Precision: 0.9354\n",
      "Recall:    0.9417\n",
      "F1-score:  0.9378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ✅ Step 1: Load CSV\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")  # Replace with your filename if different\n",
    "\n",
    "# ✅ Step 2: Define the target column\n",
    "target_col = 'category'\n",
    "\n",
    "# ✅ Step 3: Clean the label values\n",
    "df[target_col] = df[target_col].astype(str).str.strip()\n",
    "\n",
    "# ✅ Step 4: Drop image_name column (it's metadata, not a feature)\n",
    "df = df.drop(columns=['image_name'])\n",
    "\n",
    "# ✅ Step 5: Remove rare classes (those with only 1 sample)\n",
    "class_counts = df[target_col].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[target_col].isin(valid_classes)]\n",
    "\n",
    "# ✅ Step 6: Extract features and target\n",
    "X = filtered_df.drop(columns=[target_col])\n",
    "y = filtered_df[target_col]\n",
    "\n",
    "# ✅ Step 7: Define attention module\n",
    "class EnhancedWeatherAttention(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features // 2, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.FloatTensor(x)\n",
    "        att1_weights = self.att1(x)\n",
    "        x = x * att1_weights\n",
    "        att2_weights = self.att2(x)\n",
    "        x = x * att2_weights + x\n",
    "        return x\n",
    "\n",
    "# ✅ Step 8: Stacking Ensemble Class\n",
    "class StackingWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.attention = EnhancedWeatherAttention(n_features)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        self.base_models = [\n",
    "            ('svm', SVC(kernel='rbf', C=10, probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=7, random_state=42)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), early_stopping=True))\n",
    "        ]\n",
    "        \n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=10, max_iter=1000),\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        self.attention.train()\n",
    "        optimizer = torch.optim.Adam(self.attention.parameters(), lr=0.001)\n",
    "\n",
    "        for _ in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            X_att = self.attention(X_tensor)\n",
    "            loss = F.mse_loss(X_att, X_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        \n",
    "        self.meta_learner.fit(X_att_np, y_encoded)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        with torch.no_grad():\n",
    "            X_att_np = self.attention(X_tensor).numpy()\n",
    "        return self.meta_learner.predict_proba(X_att_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "\n",
    "# ✅ Step 9: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Train and Evaluate\n",
    "weather_ensemble = StackingWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# ✅ Step 11: Metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Ensemble Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6692846-0eec-47c9-bb7a-df3f15148d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Stacking Results:\n",
      "Accuracy:  0.9507\n",
      "Precision: 0.9473\n",
      "Recall:    0.9507\n",
      "F1-score:  0.9485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(\"D:/fy_project1/data4.csv\")\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "df = df.drop(columns=[\"image_name\"])\n",
    "\n",
    "# Step 2: Filter rare classes\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "filtered_df = df[df[\"category\"].isin(valid_classes)]\n",
    "\n",
    "# Step 3: Extract features and labels\n",
    "X = filtered_df.drop(columns=[\"category\"])\n",
    "y = filtered_df[\"category\"]\n",
    "\n",
    "# Step 4: Updated HybridCNNTransformer\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128, nhead=4, dim_feedforward=256, dropout=0.1,\n",
    "            activation=\"gelu\", batch_first=False, norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=3)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        x = self.conv(x)    # (batch, 128, features//4)\n",
    "        x = x.permute(2, 0, 1)  # (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights + x\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Step 5: Sklearn wrapper\n",
    "class HybridWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=50, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.model = HybridCNNTransformer(input_dim=self.input_dim, num_classes=self.num_classes_)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y_enc)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        check_is_fitted(self, ['model'])\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "        return F.softmax(outputs, dim=1).numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))\n",
    "\n",
    "# Step 6: Ensemble definition\n",
    "class ImprovedWeatherEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_features=14):\n",
    "        self.n_features = n_features\n",
    "        self.base_models = [\n",
    "            ('hybrid_cnn_transformer', HybridWrapper(n_features)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=500, max_depth=9, learning_rate=0.05,\n",
    "                tree_method='hist', eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('mlp', MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128),\n",
    "                activation='relu',\n",
    "                early_stopping=True,\n",
    "                batch_size=256\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_learner = StackingClassifier(\n",
    "            estimators=self.base_models,\n",
    "            final_estimator=LogisticRegression(C=15, max_iter=2000),\n",
    "            cv=5,\n",
    "            stack_method='auto',\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.meta_learner.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.meta_learner.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.meta_learner.predict(X)\n",
    "\n",
    "# Step 7: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 8: Train and evaluate\n",
    "weather_ensemble = ImprovedWeatherEnsemble(n_features=X.shape[1])\n",
    "weather_ensemble.fit(X_train, y_train)\n",
    "y_pred = weather_ensemble.predict(X_test)\n",
    "\n",
    "# Step 9: Print metrics\n",
    "print(f'''\n",
    "Enhanced Stacking Results:\n",
    "Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
    "Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\n",
    "Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\n",
    "F1-score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fba57-6266-4886-a227-148beb4cb051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
